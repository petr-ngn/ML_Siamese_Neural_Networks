{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.backend import epsilon\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.math import square, maximum, reduce_mean, sqrt, reduce_sum\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization, Lambda\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters' initialization\n",
    "random_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(sample_name, separate = True):\n",
    "    \n",
    "    final_df = pd.read_csv(f'./csv/{sample_name}_pairs.csv')\n",
    "\n",
    "    if separate:\n",
    "        for col in ['img_1', 'img_2']:\n",
    "            final_df[col] =  [f'./cropped_images/{i}'for i in final_df[col]]\n",
    "\n",
    "        imgs = final_df[['img_1', 'img_2']]\n",
    "        labels = final_df[['label']]\n",
    "\n",
    "        return np.array(imgs), np.array(labels)\n",
    "        \n",
    "    else:\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, train_labels = read_pairs('train')\n",
    "valid_imgs, valid_labels = read_pairs('valid')\n",
    "test_imgs, test_labels = read_pairs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4050\n",
      "1250\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "print(len(train_imgs))\n",
    "print(len(valid_imgs))\n",
    "print(len(test_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_img_pipeline(anchor, comparison):\n",
    "    \n",
    "    def tf_img_processing(img_path):\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels = 3)\n",
    "        img = tf.image.resize(img, [224,224], method = 'bilinear')\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32) /  tf.constant(255, dtype = tf.float32)\n",
    "\n",
    "        return img\n",
    "\n",
    "    return tf_img_processing(anchor), tf_img_processing(comparison)\n",
    "\n",
    "def tf_label_pipeline(label):\n",
    "    return tf.cast(label, tf.float32)\n",
    "\n",
    "def tf_data_processing_pipeline(images, labels):\n",
    "\n",
    "    images_tf = tf.data.Dataset.from_tensor_slices((images[:, 0] , images[:, 1])).map(tf_img_pipeline)\n",
    "    labels_tf = tf.data.Dataset.from_tensor_slices(labels).map(tf_label_pipeline)\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((images_tf,\n",
    "                                    labels_tf)).batch(2,\n",
    "                                                      num_parallel_calls = AUTOTUNE).cache().prefetch(buffer_size = AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = tf_data_processing_pipeline(train_imgs, train_labels)\n",
    "valid_tf = tf_data_processing_pipeline(valid_imgs, valid_labels)\n",
    "test_tf = tf_data_processing_pipeline(test_imgs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "625\n",
      "625\n"
     ]
    }
   ],
   "source": [
    "print(tf.data.experimental.cardinality(train_tf).numpy())\n",
    "print(tf.data.experimental.cardinality(valid_tf).numpy())\n",
    "print(tf.data.experimental.cardinality(test_tf).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculation an Euclidean distance between the two feature vectors\n",
    "def euclidean_distance(vectors):\n",
    "\n",
    "    x, y = vectors\n",
    "    sum_square = reduce_sum(square(x - y), axis = 1, keepdims = True)\n",
    "\n",
    "    return sqrt(maximum(sum_square, epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "#Function for a calculation of a contrastive loss\n",
    "def contrastive_loss(margin = 1):\n",
    "\n",
    "    def contrastive__loss(y_true, y_pred):\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive__loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building(hp):\n",
    "\n",
    "    #Input layer\n",
    "    inputs = Input(shape = (224, 224, 3))\n",
    "    x = inputs\n",
    "\n",
    "    #Tuning a number of convolutional blocks\n",
    "    for i in range(hp.Int('conv_blocks', min_value = 2, max_value = 5, default = 3)):\n",
    "    \n",
    "        #Tuning the number of convolution's output filters\n",
    "        filters = hp.Int('filters_' + str(i), min_value = 32,\n",
    "                        max_value = 1000, step = 32) \n",
    "\n",
    "        #Within each block, perform 2 convolutions and batch normalization\n",
    "        for _ in range(2):\n",
    "\n",
    "        #Tuning the number of convolution's output filters\n",
    "            x = Conv2D(filters, kernel_size=(3, 3), padding = 'same',\n",
    "                 activation = 'relu')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        #Tuning the pooling type in the convolutional block\n",
    "        if hp.Choice('pooling_' + str(i), ['avg', 'max']) == 'max':\n",
    "            x = MaxPooling2D()(x)\n",
    "        else:\n",
    "            x = AveragePooling2D()(x)\n",
    "    \n",
    "    #Tuning the dropout rate in the dropout layer in the convolutional block\n",
    "        x = Dropout((hp.Float('dropout', 0, 0.5, step = 0.05, default = 0.5)), seed = 123)(x)\n",
    "\n",
    "    #Flatten the output\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    #Tuning the number of units in the dense layer\n",
    "    x = Dense(hp.Int('Dense units' ,min_value = 50,\n",
    "                   max_value = 100, step = 10, default = 50),\n",
    "                  activation='relu')(x)\n",
    "\n",
    "    #Tuning the dropout rate in the dropout layer - the final feature vector layer\n",
    "    feature_layer = Dropout((hp.Float('dropout', 0, 0.5, step = 0.05, default = 0.5)), seed = 123)(x)\n",
    "\n",
    "    #Mapping a embedding model\n",
    "    embedding_network = Model(inputs, feature_layer)\n",
    "    \n",
    "    #Setting an input layer for the image pairs\n",
    "    input_1 = Input((224, 224, 3))\n",
    "    input_2 = Input((224, 224, 3))\n",
    "    tower_1 = embedding_network(input_1)\n",
    "    tower_2 = embedding_network(input_2)\n",
    "\n",
    "    #Layers for calculation of the Euclidean distance between the two feature vectors, with further normalization\n",
    "    merge_layer = Lambda(euclidean_distance)([tower_1, tower_2])\n",
    "    normal_layer = BatchNormalization()(merge_layer)\n",
    "\n",
    "    #Final output layer (classification whether the images are of the same label/person)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "\n",
    "    #Final model mapping\n",
    "    model = Model(inputs=[input_1, input_2], outputs = output_layer)\n",
    "\n",
    "    #Model compilation:\n",
    "        #Tuning the learning rate of the stochastic gradient method in the Adam optimizer.\n",
    "        #Minimizing a binary cross entropy loss function and maximizing an accuracy.\n",
    "        #We compute the binary cross entropy for each label separately and then sum them up for the complete loss.\n",
    "        \n",
    "    model.compile(optimizer = Adam(hp.Float('learning_rate', min_value = 1e-4,\n",
    "                                                            max_value = 1e-2,\n",
    "                                                            sampling = 'log')), \n",
    "                    loss = contrastive_loss(margin = 1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\untitled_project\\oracle.json\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |?                 |conv_blocks\n",
      "960               |?                 |filters_0\n",
      "avg               |?                 |pooling_0\n",
      "0.3               |?                 |dropout\n",
      "64                |?                 |filters_1\n",
      "avg               |?                 |pooling_1\n",
      "608               |?                 |filters_2\n",
      "max               |?                 |pooling_2\n",
      "50                |?                 |Dense units\n",
      "0.00066812        |?                 |learning_rate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bayes_opt = kt.tuners.BayesianOptimization(model_building, objective = 'val_loss', max_trials = 5, seed = random_seed)\n",
    "\n",
    "bayes_opt.search(train_tf,\n",
    "                 validation_data = valid_tf,\n",
    "                 epochs = 1,\n",
    "                 callbacks = [EarlyStopping(patience = 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VSE_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf7e70c757e4f60095653c44545a762e49c6e5d3353dc968e17e829e1045004e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
